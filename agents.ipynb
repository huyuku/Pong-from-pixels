{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class BasicAgent():\n",
    "    '''\n",
    "    This agent uses a 1-hidden-layer dense NN to compute probability of going UP.\n",
    "    '''\n",
    "    def __init__(self, learning_rate):\n",
    "        '''\n",
    "        learning_rate controls the learning rate of the gradient descent optimiser.\n",
    "        '''\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "            return tf.Variable(initial)\n",
    "\n",
    "        self.W1 = weight_variable([80*80, hidden_size_1])\n",
    "        self.W2 = weight_variable([hidden_size_1, 1])\n",
    "\n",
    "        self.input_vectors = tf.placeholder(shape=(None, 80*80), dtype=tf.float32)  # flattened diff_frame\n",
    "        self.actions       = tf.placeholder(shape=(None,1),      dtype=tf.float32)  # 1 if agent went UP, 0 otherwise\n",
    "        self.rewards       = tf.placeholder(shape=(None,1),      dtype=tf.float32)  # 1 if frame comes from a won game, -1 otherwise\n",
    "\n",
    "        self.hidden_layer = tf.nn.relu(tf.matmul(self.input_vectors, self.W1))\n",
    "        self.output_layer = tf.nn.sigmoid(tf.matmul(hidden_layer, self.W2))\n",
    "\n",
    "        # loss = - sum over i of reward_i * p(action_i | frame_i)\n",
    "        self.loss = tf.reduce_sum(self.rewards * (self.actions * self.output_layer + (1-self.actions)*(1-self.output_layer)))\n",
    "\n",
    "        self.GD = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    def action(self, sess, diff_frame):\n",
    "        '''returns a probability of going UP at this frame'''\n",
    "        feed_dict = {self.input_vectors:diff_frame}\n",
    "        predicted_action = sess.run(self.output_layer, feed_dict=feed_dict)[0,0]\n",
    "        action = 3 + np.random.binomial(1, predicted_action)\n",
    "        return action\n",
    "\n",
    "    def gym_action(self, sess, diff_frame):\n",
    "        return 3 + self.action(self, sess, diff_frame)\n",
    "\n",
    "    def train(self, sess, diff_frames, actions, wins):\n",
    "        '''trains the agent on the data'''\n",
    "        feed_dict={self.input_vector:diff_frames, self.actions:actions, self.rewards:wins}\n",
    "        _, loss = sess.run([self.GD.self.minimize(self.loss), self.loss], feed_dict=feed_dict)\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
